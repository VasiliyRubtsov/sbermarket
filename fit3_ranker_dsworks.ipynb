{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cc4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIR = 'input'\n",
    "\n",
    "TRAIN_VAL_PATH = os.path.join(DIR, 'train_val1.parquet')  \n",
    "VAL_PATH = os.path.join(DIR, 'val1.parquet') \n",
    "RECS_VAL_PATH = os.path.join(DIR, 'recs_val1.parquet') \n",
    "\n",
    "TOPK_VAL_PATH = os.path.join(DIR, 'topk_val.parquet')\n",
    "TOPK_VAL1_PATH = os.path.join(DIR, 'topk_val1.parquet')\n",
    "TOPK_VAL2_PATH = os.path.join(DIR, 'topk_val2.parquet')\n",
    "\n",
    "CLUSTERS_PATH = os.path.join(DIR, 'clusters.parquet')  \n",
    "USER_DECODER_PATH = os.path.join(DIR, 'user_decoder.pkl') \n",
    "RANKER_MODEL_PATH = os.path.join(DIR, 'ranker_model.pkl')\n",
    "RANKER_MODEL1_PATH = os.path.join(DIR, 'ranker_model1.pkl')\n",
    "RANKER_MODEL2_PATH = os.path.join(DIR, 'ranker_model2.pkl')\n",
    "\n",
    "RECS_NN_VAL_PATH = os.path.join(DIR, 'recs_nn_val1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8970414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_CLUSTERS = [ \n",
    "    937, 6849, 4873, 7052,  789, 4938, 5998, 5124, 4083,  345,  721,\n",
    "    4018, 6995, 3334, 4327, 7401, 3684,  292, 7454, 5452, 1023, 6674,\n",
    "    3366, 4236, 6983, 4647, 2214, 2895, 3205, 4031, 2578,   42, 7855,\n",
    "    931, 3107, 2000, 7532, 6761, 1131, 3717, 2351, 2728, 4929, 3027,\n",
    "    612,   21, 1902,  807, 4001, 3771, 1705,  602, 1020, 6428, 6699,\n",
    "    6271,  554, 4308, 7589, 7002, 1997,  696,  595, 6675, 1751,  923,\n",
    "    6711,  999, 1666, 1263,  919, 7602, 2285, 4543, 6051, 4540, 4828,\n",
    "    3543, 6928, 1886, 6029, 5320, 2924, 7449, 4906, 7757, 1077, 5378,\n",
    "    6189, 1747, 7691, 2595,  811,  103, 7043, 1339, 1574, 2570, 1249,\n",
    "    735, 3173, 4739, 2152, 2226, 6021, 7739, 7777, 5187, 5299, 2604,\n",
    "    6569, 5893,  466, 3483, 3640, 3870, 1442, 7114, 1338, 7747, 1867,\n",
    "    2702, 3046, 1182, 1409, 4663, 4932, 1570, 6053, 6071, 3733,  712,\n",
    "    3549, 6668, 1006, 4358, 4285, 3668,  885, 4129, 3293,  407, 4392,\n",
    "    3555, 5812,  129,  163, 3018, 7752, 6998, 5949, 1266, 6656, 2786,\n",
    "    2199, 2644, 4201, 3514, 6147, 4426, 7495, 5096, 5653,  341, 1826,\n",
    "    5380,  587, 4062, 6069, 2881, 1377, 6548, 2685, 2629, 7028, 6831,\n",
    "    7181, 3251, 3948, 1357, 4438, 1138, 7528, 6149, 7514, 4835, 3938,\n",
    "    1932, 3358, 2503,   11, 1623, 4028, 1890, 6696,  354,  960, 1765,\n",
    "    3699, 7636,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301f5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def most_common(array):\n",
    "    elements, counts = np.unique(array, return_counts=True)\n",
    "    return elements[np.argpartition(counts, kth=-1)[-1]]\n",
    "\n",
    "def apply_rank(col, df):\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    _, index, num_ranges = np.unique(df[col], return_counts=True, return_index=True)\n",
    "    num_ranges = num_ranges[index.argsort()]\n",
    "    arange = np.arange(num_ranges.max(), dtype=int)\n",
    "    ranks = np.hstack([arange[:i] for i in num_ranges])\n",
    "    return ranks\n",
    "\n",
    "def get_mean_diff_dt(array):\n",
    "    if len(array) == 1:\n",
    "        return -1\n",
    "    np_array = np.array(array)\n",
    "    np_array[1:] - np_array[:-1]\n",
    "    return (np_array[1:] - np_array[:-1]).mean()\n",
    "    \n",
    "def create_features_simple(table, train, users, clusters):\n",
    "    table['count_item_id'] = (table.cluster_id.map(train['cluster_id'].value_counts()).fillna(0) / len(train)).astype(np.float32)\n",
    "\n",
    "    table['num_orders'] = table['user_id'].map(\n",
    "        train[['order_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    table['num_order_with_target_item'] = table['ui'].map(\n",
    "        train[['order_id', 'ui']].drop_duplicates()['ui'].value_counts()\n",
    "    ).fillna(0).astype(np.int16)\n",
    "    \n",
    "    last_order_ui = train[train.dt == \\\n",
    "          train['user_id'].map(\n",
    "                train[['user_id', 'dt']].drop_duplicates().groupby('user_id').max()['dt']\n",
    "    )].ui.unique()\n",
    "\n",
    "    table['was_in_last_order'] = table['ui'].isin(last_order_ui).astype(np.int8)\n",
    "    del last_order_ui\n",
    "\n",
    "    prod_quantity = train.groupby('ui')['product_quantity'].sum()\n",
    "    table['prod_quantity'] = table['ui'].map(prod_quantity).fillna(0).astype(np.int16)\n",
    "    del prod_quantity\n",
    "    \n",
    "    prev_order_ui = train['dt'].max() - train.groupby('ui')['dt'].max()\n",
    "    table['prev_order_ui'] = table['ui'].map(prev_order_ui).fillna(-1).astype(np.float32)\n",
    "    del prev_order_ui\n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id']].duplicated()\n",
    "    \n",
    "    table['user_retailer_most_common'] = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').retailer_id.apply(most_common)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    user_city_most_common = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').city_id.apply(most_common)\n",
    "    ).astype(np.int16)\n",
    "    \n",
    "    del mask\n",
    "    \n",
    "    item_city_vc = (train['cluster_id'] * 100 + train['city_id']).value_counts()\n",
    "    item_user_city = table['cluster_id'] * 100 + user_city_most_common\n",
    "    table['user_item_city_vc'] = item_user_city.map(item_city_vc).fillna(0).astype(np.float32)\n",
    "    del item_city_vc\n",
    "    del item_user_city\n",
    "    \n",
    "    for col in ['cluster_size', 'd_mean', 'd_median']:\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        \n",
    "    short_train = train[train.user_id.isin(users)]\n",
    "    \n",
    "    table['product_quantity_sum'] = table.user_id.map(\n",
    "          short_train.groupby('user_id').product_quantity.sum()\n",
    "    )\n",
    "    table['user_retailer_num'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').retailer_id.nunique()\n",
    "    ).astype(np.int8)\n",
    "    table['user_city_num'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').city_id.nunique()\n",
    "    ).astype(np.int8)\n",
    "    table['user_product_price_mean'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').product_price.mean()\n",
    "    )\n",
    "    table['user_product_discount_mean'] = table.user_id.map(\n",
    "        (short_train.product_discount != 0).groupby(short_train.user_id).mean()\n",
    "    ).astype(np.float16)\n",
    "    table['user_num_clusters'] = table['user_id'].map(\n",
    "        short_train[['cluster_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "    table['last_user_city_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').city_id.last()\n",
    "    )\n",
    "    table['last_user_retailer_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').retailer_id.last()\n",
    "    )\n",
    "    \n",
    "    table['user_most_common_cluster_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').cluster_id.apply(most_common)\n",
    "    )\n",
    "    del short_train\n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id', 'cluster_id']].duplicated()\n",
    "\n",
    "    table['cluster_quantity_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_quantity.mean().astype(np.float16)\n",
    "    )\n",
    "\n",
    "    table['cluster_city_count'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').city_id.nunique()\n",
    "    ).astype(np.float16)\n",
    "    \n",
    "    table['cluster_num_stores'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').store_id.nunique()\n",
    "    ).astype(np.float16)\n",
    "    del mask\n",
    "\n",
    "    table['cluster_product_price_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_price.mean()\n",
    "    ).astype(np.float16)\n",
    "\n",
    "    table['cluster_mean_discount'] = table['cluster_id'].map(\n",
    "        (train.product_discount == 0).groupby(train.cluster_id).mean().astype(np.float16)\n",
    "    )\n",
    "\n",
    "    table['num_users_bought_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').user_id.nunique()\n",
    "    ).fillna(0).astype(np.float16)\n",
    "\n",
    "    table['num_orders_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').order_id.nunique()\n",
    "    ).fillna(0).astype(np.float16)\n",
    "    \n",
    "    mask = ~train[['order_id', 'cluster_id']].duplicated()\n",
    "    short_train = train[mask]\n",
    "\n",
    "    city_retailer = short_train.city_id.astype(np.int16) * 100 + short_train.retailer_id\n",
    "    city_retailer_cluster = city_retailer.astype(np.int64) * 10000 + short_train.cluster_id\n",
    "\n",
    "    city_retailer_user = user_city_most_common.astype(np.int16) * 100 + \\\n",
    "        table['user_retailer_most_common']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    table['f1'] = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f2'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f3'] = table['f2'] \\\n",
    "        / table['f1'] \n",
    "    \n",
    "    \n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "\n",
    "    city_retailer_user = table['last_user_city_id'].astype(np.int16) * 100 + \\\n",
    "        table['last_user_retailer_id']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    f4 = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f5'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f6'] = table['f5'] \\\n",
    "        / f4 \n",
    "    del f4\n",
    "    \n",
    "    del city_retailer\n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "    del city_retailer_cluster\n",
    "\n",
    "    ui_vc = train.ui.value_counts()\n",
    "    rnk_vc = train[['user_id', 'ui', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['vc'] = rnk_vc.ui.map(ui_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['user_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_user_id_ui'] = apply_rank('user_id', rnk_vc)\n",
    "    table['rnk_user_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_user_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del ui_vc\n",
    "\n",
    "    rnk_vc = rnk_vc.sort_values(['cluster_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_id_ui'] = apply_rank('cluster_id', rnk_vc)\n",
    "    table['rnk_cluster_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_cluster_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    rnk_vc = train['cluster_id'].value_counts().to_frame()\n",
    "    rnk_vc['rnk_cluster_id'] = np.arange(len(rnk_vc))\n",
    "    table['rnk_cluster_id'] = table.cluster_id.map(rnk_vc['rnk_cluster_id']\n",
    "                                                  ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    cluster_city_vc = (train['city_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['city_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_city'] = rnk_vc['city_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_city'].map(cluster_city_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['city_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_city'] = apply_rank('city_id', rnk_vc)\n",
    "    user_city_cluster = table['last_user_city_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_city'] = user_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_city')['rnk_cluster_city']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_city_vc\n",
    "    del rnk_vc\n",
    "    del user_city_cluster\n",
    "\n",
    "    cluster_retailer_vc = (train['retailer_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer'] = rnk_vc['retailer_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer'].map(cluster_retailer_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_retailer'] = apply_rank('retailer_id', rnk_vc)\n",
    "    user_retailer_cluster = table['last_user_retailer_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_cluster = table['user_retailer_most_common'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer2'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_cluster\n",
    "\n",
    "    cluster_retailer_city_vc = (train['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        train['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        train['cluster_id']).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id', 'city_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        rnk_vc['cluster_id'])\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer_city'].map(cluster_retailer_city_vc)\n",
    "    rnk_vc['retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 1000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64))\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_city', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluser_city_retailer'] = apply_rank('retailer_city', rnk_vc)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['last_user_retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['user_retailer_most_common'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city2'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_city_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_city_cluster\n",
    "\n",
    "    \n",
    "    return table\n",
    "\n",
    "def create_table(train, recs_nn, recs_mf, users):\n",
    "    \n",
    "    recs_nn['rnk'] = apply_rank('user_id', recs_nn)\n",
    "    recs_mf['rnk'] = apply_rank('user_id', recs_mf)\n",
    "\n",
    "    mask1 = recs_nn['user_id'].isin(users)\n",
    "    mask2 = ~recs_mf.ui.isin(recs_nn.ui) & recs_mf['user_id'].isin(users)\n",
    "    mask3 = ~(train.ui.isin(recs_nn.ui) | train.ui.isin(recs_mf.ui) \\\n",
    "              | train.ui.duplicated())  & train['user_id'].isin(users)\n",
    "    \n",
    "    table = pd.concat([\n",
    "        recs_nn[['user_id', 'cluster_id']][mask1], \n",
    "        recs_mf[['user_id', 'cluster_id']][mask2], \n",
    "        train[['user_id', 'cluster_id']][mask3]\n",
    "    ])\n",
    "    table.reset_index(drop=True, inplace=True)\n",
    "    del mask1\n",
    "    del mask2\n",
    "    del mask3\n",
    "    table['ui'] = table['user_id'].astype(np.int64) * 10000 + table['cluster_id']\n",
    "    \n",
    "    table['rnk'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['scores']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    recs_nn = recs_nn[~recs_nn.ui.isin(train.ui)]\n",
    "    recs_nn['rnk2'] = apply_rank('user_id', recs_nn)\n",
    "    table['rnk2'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['rnk3'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score2'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['score']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    recs_mf = recs_mf[~recs_mf.ui.isin(train.ui)]\n",
    "    recs_mf['rnk2'] = apply_rank('user_id', recs_mf)\n",
    "    table['rnk4'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['rnk2']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    return table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92fa2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs(pred, users, items, already_bought, weights, num_recs=20):\n",
    "    fix_pred = pred * (1.37 - already_bought) * (weights ** 1.5)\n",
    "    indexes = (-fix_pred).argsort()\n",
    "    recs = defaultdict(list)\n",
    "    for user_id, item_id in zip(users[indexes], items[indexes]):\n",
    "        if len(recs[user_id]) < num_recs:\n",
    "            recs[user_id].append(item_id)\n",
    "    return recs\n",
    "\n",
    "\n",
    "def get_cluster_weights(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    cluster_popularity = dataset[\"cluster_id\"].value_counts().sort_values(ascending=True).reset_index()\n",
    "    cluster_popularity.columns = [\"cluster_id\", \"cnt\"]\n",
    "    cluster_popularity[\"rank\"] = cluster_popularity[\"cnt\"].rank(method=\"dense\") + 1\n",
    "    cluster_popularity[\"w\"] = 1 / np.log10(cluster_popularity[\"rank\"])\n",
    "\n",
    "    return cluster_popularity[[\"cluster_id\", \"w\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdf289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(train_path, recs_nn_path, recs_mf_path, \n",
    "              users, create_features_func, val_path=None):\n",
    "\n",
    "    train = pd.read_parquet(train_path)\n",
    "    train['product_price'] = train['product_price'].astype(np.float16)\n",
    "    train['product_discount'] = train['product_discount'].astype(np.float16)\n",
    "\n",
    "    recs_nn = pd.read_parquet(recs_nn_path)\n",
    "    recs_mf = pd.read_parquet(recs_mf_path)\n",
    "    clusters = pd.read_parquet(CLUSTERS_PATH)\n",
    "    \n",
    "    for df in [train, recs_nn, recs_mf]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "    \n",
    "    table = create_table(train, recs_nn, recs_mf, users)\n",
    "    del recs_nn\n",
    "    del recs_mf\n",
    "\n",
    "    table = create_features_func(table, train, users, clusters)\n",
    "    del train\n",
    "    del clusters\n",
    "\n",
    "    X = table.drop(['user_id', 'ui'], axis=1).to_numpy(dtype=np.float32)\n",
    "\n",
    "    if val_path is None:\n",
    "        return X\n",
    "    \n",
    "    val = pd.read_parquet(val_path)\n",
    "    val['ui'] = val['user_id'].astype(np.int64) * 10000 + val['cluster_id']\n",
    "    y = np.array(table['ui'].isin(val['ui']))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3083bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_first_ranker(ranker_model_path, users):\n",
    "\n",
    "    X, y = get_table(TRAIN_VAL_PATH, RECS_NN_VAL_PATH, RECS_VAL_PATH, \n",
    "                         users, create_features_simple, VAL_PATH)\n",
    "    \n",
    "    lgb_table = lightgbm.Dataset(X, y)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.1,\n",
    "        'verbose': 0,\n",
    "        'seed': 1,\n",
    "    }\n",
    "\n",
    "    gbm = lightgbm.train(params, lgb_table, num_boost_round=1000)\n",
    "    \n",
    "    pickle.dump(gbm, open(ranker_model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd53e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_layer_models():\n",
    "    val = pd.read_parquet(VAL_PATH)\n",
    "    users = val.user_id.unique()\n",
    "    del val\n",
    "    users1 = users[users % 2 == 0]\n",
    "    users2 = users[users % 2 == 1]\n",
    "    del users\n",
    "    fit_first_ranker(RANKER_MODEL1_PATH, users1)\n",
    "    fit_first_ranker(RANKER_MODEL2_PATH, users2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ee49c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /opt/intel/oneapi/intelpython/python3.7/envs/aikit/lib/python3.7/site-packages (0.60.0)\r\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/python3.7/envs/aikit/lib/python3.7/site-packages (from memory_profiler) (5.8.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0c1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fae4a75",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 5.430081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 4.286239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "peak memory: 30635.54 MiB, increment: 30448.21 MiB\n",
      "CPU times: user 6h 40min 19s, sys: 6min 44s, total: 6h 47min 3s\n",
      "Wall time: 1h 16min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "create_first_layer_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f0e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_data(train_path, recs_nn_path, recs_mf_path, users):\n",
    "    \n",
    "    train = pd.read_parquet(train_path)\n",
    "    recs_nn = pd.read_parquet(recs_nn_path)\n",
    "    recs_mf = pd.read_parquet(recs_mf_path)\n",
    "    clusters = pd.read_parquet(CLUSTERS_PATH)\n",
    "    \n",
    "    for df in [train, recs_nn, recs_mf]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "    \n",
    "    table = create_table(train, recs_nn, recs_mf, users)\n",
    "    \n",
    "    already_bought = np.array(table['ui'].isin(train['ui']))\n",
    "    cluster_weights = get_cluster_weights(train)\n",
    "    weights = np.array(table.cluster_id.map(\n",
    "        cluster_weights.set_index('cluster_id')['w']\n",
    "    ).fillna(cluster_weights['w'].max()))\n",
    "    del cluster_weights\n",
    "    \n",
    "    return (\n",
    "        np.array(table['user_id']), \n",
    "        np.array(table['cluster_id']), \n",
    "        already_bought, \n",
    "        weights\n",
    "    )\n",
    "\n",
    "def _create_top_k(train_path, recs_nn_path, recs_mf_path, \n",
    "                 users, model_path, top_k_path, model_path2=None, k=60):\n",
    "    \n",
    "    X = get_table(train_path, recs_nn_path, recs_mf_path, users, create_features_simple)\n",
    "    print(0)\n",
    "    if model_path2 is None:\n",
    "        ranker_model = pickle.load(open(model_path, 'rb'))\n",
    "        pred = ranker_model.predict(X)\n",
    "    else:\n",
    "        ranker_model = pickle.load(open(model_path, 'rb'))\n",
    "        pred1 = ranker_model.predict(X)\n",
    "        ranker_model = pickle.load(open(model_path2, 'rb'))\n",
    "        pred2 = ranker_model.predict(X)\n",
    "        pred = np.mean([pred1, pred2], axis=0)\n",
    "        \n",
    "    del X\n",
    "    print(1)\n",
    "    users, items, already_bought, weights = get_some_data(\n",
    "        train_path, recs_nn_path, recs_mf_path, users\n",
    "    )\n",
    "    recs = get_recs(pred, users, items, already_bought, weights, num_recs=k)\n",
    "    \n",
    "    users = []\n",
    "    items = []\n",
    "    for user_id in recs:\n",
    "        users += [user_id] * len(recs[user_id])\n",
    "        items += recs[user_id]\n",
    "    del recs\n",
    "\n",
    "    top_k = pd.DataFrame()\n",
    "    top_k['user_id'] = users\n",
    "    top_k['cluster_id'] = items\n",
    "    top_k.to_parquet(top_k_path)\n",
    "\n",
    "def create_top_k():\n",
    "    \n",
    "    val = pd.read_parquet(VAL_PATH)\n",
    "    users = val.user_id.unique()\n",
    "    del val\n",
    "    users1 = users[users % 2 == 0]\n",
    "    users2 = users[users % 2 == 1]\n",
    "    del users\n",
    "    \n",
    "    _create_top_k(TRAIN_VAL_PATH, RECS_NN_VAL_PATH, RECS_VAL_PATH,\n",
    "             users1, RANKER_MODEL2_PATH, TOPK_VAL1_PATH)\n",
    "    _create_top_k(TRAIN_VAL_PATH, RECS_NN_VAL_PATH, RECS_VAL_PATH,\n",
    "         users2, RANKER_MODEL1_PATH, TOPK_VAL2_PATH)\n",
    "    \n",
    "    topk_val1 = pd.read_parquet(TOPK_VAL1_PATH)\n",
    "    topk_val2 = pd.read_parquet(TOPK_VAL2_PATH)\n",
    "    topk_val = pd.concat([topk_val1, topk_val2])\n",
    "    topk_val.to_parquet(TOPK_VAL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ef1f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 30802.16 MiB, increment: 29788.19 MiB\n",
      "CPU times: user 3h 40min 29s, sys: 4min 19s, total: 3h 44min 48s\n",
      "Wall time: 1h 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "create_top_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_table(train, table, clusters, recs_nn_path, recs_mf_path):\n",
    "    \n",
    "    users = table.user_id.unique()\n",
    "    \n",
    "    recs_nn = pd.read_parquet(recs_nn_path)\n",
    "    recs_mf = pd.read_parquet(recs_mf_path)\n",
    "    \n",
    "    for df in [train, recs_nn, recs_mf]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "        \n",
    "    recs_nn['rnk'] = apply_rank('user_id', recs_nn)\n",
    "    recs_mf['rnk'] = apply_rank('user_id', recs_mf)\n",
    "    \n",
    "    table['rnk'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['scores']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    mask = recs_nn.ui.isin(train.ui)\n",
    "    \n",
    "    recs_short = recs_nn[~mask]\n",
    "    recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "    table['rnk2'] = table['ui'].map(\n",
    "        recs_short.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "#     recs_short = recs_nn[mask]\n",
    "#     recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "#     table['rnk3'] = table['ui'].map(\n",
    "#         recs_short.set_index('ui')['rnk']\n",
    "#     ).fillna(10000).astype(np.int16)\n",
    "    del recs_nn\n",
    "    \n",
    "    table['rnk4'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "#     table['score2'] = table['ui'].map(\n",
    "#         recs_mf.set_index('ui')['score']\n",
    "#     ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    mask = recs_mf.ui.isin(train.ui)\n",
    "    \n",
    "    recs_short = recs_mf[~mask]\n",
    "    recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "    table['rnk5'] = table['ui'].map(\n",
    "        recs_short.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "#     recs_short = recs_mf[mask]\n",
    "#     recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "#     table['rnk6'] = table['ui'].map(\n",
    "#         recs_short.set_index('ui')['rnk']\n",
    "#     ).fillna(10000).astype(np.int16)\n",
    "    del recs_mf\n",
    "    \n",
    "#     count_user_id = table.user_id.map(train['user_id'].value_counts()).fillna(0).astype(np.int16)\n",
    "    table['count_item_id'] = (table.cluster_id.map(train['cluster_id'].value_counts()).fillna(0) / len(train)).astype(np.float32)\n",
    "\n",
    "    table['num_orders'] = table['user_id'].map(\n",
    "        train[['order_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    table['num_order_with_target_item'] = table['ui'].map(\n",
    "        train[['order_id', 'ui']].drop_duplicates()['ui'].value_counts()\n",
    "    ).fillna(0).astype(np.int16)\n",
    "    \n",
    "    last_order_ui = train[train.dt == \\\n",
    "          train['user_id'].map(\n",
    "                train[['user_id', 'dt']].drop_duplicates().groupby('user_id').max()['dt']\n",
    "    )].ui.unique()\n",
    "\n",
    "    table['was_in_last_order'] = table['ui'].isin(last_order_ui).astype(np.int8)\n",
    "    del last_order_ui\n",
    "\n",
    "    prod_quantity = train.groupby('ui')['product_quantity'].sum()\n",
    "    table['prod_quantity'] = table['ui'].map(prod_quantity).fillna(0).astype(np.int16)\n",
    "    del prod_quantity\n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id']].duplicated()\n",
    "    \n",
    "    table['user_retailer_most_common'] = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').retailer_id.apply(most_common)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    user_city_most_common = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').city_id.apply(most_common)\n",
    "    ).astype(np.int16)\n",
    "    \n",
    "    del mask\n",
    "    \n",
    "#     item_retailer_vc = (train['cluster_id'] * 100 + train['retailer_id']).value_counts()\n",
    "#     item_user_retailer = table['cluster_id'] * 100 + table['user_retailer_most_common']\n",
    "#     table['user_item_retailer_vc'] = item_user_retailer.map(item_retailer_vc).fillna(0).astype(np.float32)\n",
    "#     del item_retailer_vc\n",
    "#     del item_user_retailer\n",
    "    \n",
    "    item_city_vc = (train['cluster_id'] * 100 + train['city_id']).value_counts()\n",
    "    item_user_city = table['cluster_id'] * 100 + user_city_most_common\n",
    "    table['user_item_city_vc'] = item_user_city.map(item_city_vc).fillna(0).astype(np.float32)\n",
    "    del item_city_vc\n",
    "    del item_user_city\n",
    "    \n",
    "    for col in ['cluster_size', 'd_mean', 'd_median']:\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        \n",
    "#     # user features \n",
    "    short_train = train[train.user_id.isin(users)]\n",
    "    \n",
    "    ui_dt = defaultdict(list)\n",
    "    short_train3 = short_train[~short_train[['ui', 'order_id']].duplicated()]\n",
    "    for ui, dt in zip(short_train3['ui'], short_train3['dt']):\n",
    "        ui_dt[ui].append(dt)\n",
    "    del short_train3\n",
    "    table['ui_dt_diff_mean'] = table.ui.map(\n",
    "        {key: get_mean_diff_dt(value) for key, value in ui_dt.items()}\n",
    "    ).fillna(-1).astype(np.float32)\n",
    "    del ui_dt\n",
    "    \n",
    "    table['product_quantity_sum'] = table.user_id.map(\n",
    "          short_train.groupby('user_id').product_quantity.sum()\n",
    "    )\n",
    "    table['user_retailer_num'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').retailer_id.nunique()\n",
    "    ).astype(np.int8)\n",
    "#     table['user_city_num'] = table.user_id.map(\n",
    "#         short_train.groupby('user_id').city_id.nunique()\n",
    "#     ).astype(np.int8)\n",
    "    table['user_product_price_mean'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').product_price.mean()\n",
    "    )\n",
    "#     table['user_product_price_sum'] = table.user_id.map(\n",
    "#         short_train.product_price.astype(np.float32).groupby(short_train.user_id).sum()\n",
    "#     )\n",
    "    table['user_product_discount_mean'] = table.user_id.map(\n",
    "        (short_train.product_discount != 0).groupby(short_train.user_id).mean()\n",
    "    ).astype(np.float32)\n",
    "    table['user_num_clusters'] = table['user_id'].map(\n",
    "        short_train[['cluster_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "    table['last_user_city_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').city_id.last()\n",
    "    )\n",
    "    table['last_user_retailer_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').retailer_id.last()\n",
    "    )\n",
    "#     table['user_mean_clusters_in_order'] = table['user_id'].map(\n",
    "#         short_train.groupby(['user_id', 'order_id']).cluster_id.nunique().reset_index() \\\n",
    "#         .groupby('user_id').cluster_id.mean()\n",
    "#     ).astype(np.float16)\n",
    "    table['user_most_common_cluster_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').cluster_id.apply(most_common)\n",
    "    )\n",
    "    del short_train\n",
    "    \n",
    "    # item features \n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id', 'cluster_id']].duplicated()\n",
    "    \n",
    "#     table['cluster_quantity_sum'] = table['cluster_id'].map(\n",
    "#         train.groupby('cluster_id').product_quantity.sum().astype(np.float32)\n",
    "#     )\n",
    "    table['cluster_quantity_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_quantity.mean().astype(np.float32)\n",
    "    )\n",
    "\n",
    "    for retailer_id in [0, 1, 7]: # [1, 7, 0, 16, 6, 4, 19, 12, 15]\n",
    "        table[f'cluster_retailer_{retailer_id}'] = table['cluster_id'].map(\n",
    "            (train[mask].retailer_id == retailer_id).groupby(train[mask].cluster_id).mean(\n",
    "            ).astype(np.float32)\n",
    "        )\n",
    "    \n",
    "    table['cluster_city_count'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').city_id.nunique()\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "\n",
    "#     table['last_dt_delta'] = table['cluster_id'].map(\n",
    "#         train.dt.max() - train.groupby('cluster_id').dt.max()\n",
    "#     ).astype(np.float32)\n",
    "\n",
    "    table['cluster_num_stores'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').store_id.nunique()\n",
    "    ).astype(np.float32)\n",
    "    del mask\n",
    "\n",
    "    table['cluster_product_price_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_price.mean()\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    table['cluster_mean_discount'] = table['cluster_id'].map(\n",
    "        (train.product_discount == 0).groupby(train.cluster_id).mean().astype(np.float32)\n",
    "    )\n",
    "\n",
    "    table['num_users_bought_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').user_id.nunique()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['num_orders_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').order_id.nunique()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "    \n",
    "#     more features\n",
    "    \n",
    "    mask = ~train[['order_id', 'cluster_id']].duplicated()\n",
    "    short_train = train[mask]\n",
    "\n",
    "    city_retailer = short_train.city_id.astype(np.int16) * 100 + short_train.retailer_id\n",
    "    city_retailer_cluster = city_retailer.astype(np.int64) * 10000 + short_train.cluster_id\n",
    "\n",
    "    city_retailer_user = user_city_most_common.astype(np.int16) * 100 + \\\n",
    "        table['user_retailer_most_common']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    table['f1'] = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f2'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f3'] = table['f2'] \\\n",
    "        / table['f1'] \n",
    "    \n",
    "    \n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "\n",
    "    city_retailer_user = table['last_user_city_id'].astype(np.int16) * 100 + \\\n",
    "        table['last_user_retailer_id']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    f4 = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f5'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f6'] = table['f5'] \\\n",
    "        / f4 \n",
    "    del f4\n",
    "    \n",
    "    del city_retailer\n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "    del city_retailer_cluster\n",
    "    \n",
    "    #more and more features\n",
    "    \n",
    "    short_train = train[train.user_id.isin(users)]\n",
    "    short_train2 = short_train[~short_train[['user_id', 'order_id']].duplicated()]\n",
    "    \n",
    "    table['time_from_order_with_target_item'] = table.ui.map(\n",
    "        short_train.dt.max() - short_train.groupby('ui').dt.last()\n",
    "    ).fillna(-1).astype(np.float32)\n",
    "    \n",
    "    user_dt = defaultdict(list)\n",
    "    for user_id, dt in zip(short_train2['user_id'], short_train2['dt']):\n",
    "        user_dt[user_id].append(dt)\n",
    "    del short_train2\n",
    "    \n",
    "    table['user_dt_diff_mean'] = table.user_id.map(\n",
    "        {key: get_mean_diff_dt(value) for key, value in user_dt.items()}\n",
    "    ).fillna(-1).astype(np.float32)\n",
    "    del user_dt\n",
    "    \n",
    "    table['share_order_with_target_item'] = (\n",
    "        table['num_order_with_target_item'] / table['num_orders'] \n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    table['ui_num'] = table.ui.map(short_train.ui.value_counts()).fillna(0).astype(np.int16)\n",
    "#     table['share_clusters_with_target_item'] = (\n",
    "#         table['ui_num']/ table['count_user_id']\n",
    "#     ).astype(np.float32)\n",
    "\n",
    "    table['share_quatity'] = (\n",
    "        table['prod_quantity'] / table['product_quantity_sum']\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    short_train4 = short_train[\n",
    "        short_train.user_id.map(short_train.groupby('user_id').retailer_id.last()) == \\\n",
    "        short_train.retailer_id\n",
    "    ]\n",
    "        \n",
    "    table['num_order_with_last_retailer'] = table['user_id'].map(\n",
    "        short_train4[['user_id', 'order_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    table['num_order_with_target_item_last_retailer'] = table['ui'].map(\n",
    "        short_train4[['order_id', 'ui']].drop_duplicates()['ui'].value_counts()\n",
    "    ).fillna(0).astype(np.int16)\n",
    "    del short_train4\n",
    "\n",
    "    table['share_order_with_target_item_last_retailer'] = (\n",
    "        table['num_order_with_target_item_last_retailer'] / table['num_order_with_last_retailer']\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ui_vc = train.ui.value_counts()\n",
    "    rnk_vc = train[['user_id', 'ui', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['vc'] = rnk_vc.ui.map(ui_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['user_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_user_id_ui'] = apply_rank('user_id', rnk_vc)\n",
    "    table['rnk_user_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_user_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del ui_vc\n",
    "\n",
    "    rnk_vc = rnk_vc.sort_values(['cluster_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_id_ui'] = apply_rank('cluster_id', rnk_vc)\n",
    "    table['rnk_cluster_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_cluster_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    rnk_vc = train['cluster_id'].value_counts().to_frame()\n",
    "    rnk_vc['rnk_cluster_id'] = np.arange(len(rnk_vc))\n",
    "    table['rnk_cluster_id'] = table.cluster_id.map(rnk_vc['rnk_cluster_id']\n",
    "                                                  ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    cluster_city_vc = (train['city_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['city_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_city'] = rnk_vc['city_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_city'].map(cluster_city_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['city_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_city'] = apply_rank('city_id', rnk_vc)\n",
    "    user_city_cluster = table['last_user_city_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_city'] = user_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_city')['rnk_cluster_city']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_city_vc\n",
    "    del rnk_vc\n",
    "    del user_city_cluster\n",
    "\n",
    "    cluster_retailer_vc = (train['retailer_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer'] = rnk_vc['retailer_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer'].map(cluster_retailer_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_retailer'] = apply_rank('retailer_id', rnk_vc)\n",
    "    user_retailer_cluster = table['last_user_retailer_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_cluster = table['user_retailer_most_common'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer2'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_cluster\n",
    "\n",
    "    cluster_retailer_city_vc = (train['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        train['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        train['cluster_id']).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id', 'city_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        rnk_vc['cluster_id'])\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer_city'].map(cluster_retailer_city_vc)\n",
    "    rnk_vc['retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 1000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64))\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_city', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluser_city_retailer'] = apply_rank('retailer_city', rnk_vc)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['last_user_retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['user_retailer_most_common'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city2'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_city_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_city_cluster\n",
    "    \n",
    "    short_train = train[['cluster_id', 'user_id']][\n",
    "        train.user_id.isin(users) & (~train[['ui', 'order_id']].duplicated())\n",
    "    ]\n",
    "    \n",
    "    vc = short_train['user_id'].value_counts()\n",
    "    for cluster_id in TOP_K_CLUSTERS[:140]:\n",
    "        table[f'f102_{cluster_id}'] = table.user_id.map(\n",
    "            (short_train.cluster_id == cluster_id).groupby(short_train.user_id).sum() / vc\n",
    "        ).astype(np.float16)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69956a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_table(path1, path2, path3, path4, path5):\n",
    "\n",
    "    train_val = pd.read_parquet(path1)\n",
    "    train_val['product_price'] = train_val['product_price'].astype(np.float32)\n",
    "    train_val['product_discount'] = train_val['product_discount'].astype(np.float32)\n",
    "    val = pd.read_parquet(path2)\n",
    "    clusters = pd.read_parquet(CLUSTERS_PATH)\n",
    "    table = pd.read_parquet(path5)\n",
    "\n",
    "    for df in [train_val, val, table]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "    print(0)\n",
    "    table = create_fit_table(train_val, table, clusters,\n",
    "                            path3, path4)\n",
    "    print(1)\n",
    "    del train_val\n",
    "    del clusters\n",
    "    \n",
    "    y  = np.array(table['ui'].isin(val['ui']))\n",
    "    del val\n",
    "    print(2)\n",
    "    del table['user_id']\n",
    "    del table['ui']\n",
    "    X = table.to_numpy(dtype=np.float32)\n",
    "    del table\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_ranker():\n",
    "\n",
    "    X, y = get_lgb_table(TRAIN_VAL_PATH, VAL_PATH, RECS_NN_VAL_PATH, \n",
    "                         RECS_VAL_PATH, TOPK_VAL_PATH)\n",
    "    \n",
    "    lgb_table = lightgbm.Dataset(X, y)\n",
    "    print(3)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.03,\n",
    "        'verbose': 0,\n",
    "        'seed': 1,\n",
    "    }\n",
    "    \n",
    "    gbm = lightgbm.train(params, lgb_table, num_boost_round=5000)\n",
    "    pickle.dump(gbm, open(RANKER_MODEL_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94cb9719",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 7.791168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "peak memory: 32202.47 MiB, increment: 32014.02 MiB\n",
      "CPU times: user 9h 36min 28s, sys: 10min 39s, total: 9h 47min 7s\n",
      "Wall time: 1h 30min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit fit_ranker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
