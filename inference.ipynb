{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be69d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.10.0\n",
      "  Using cached torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/intel/oneapi/intelpython/python3.7/envs/aikit/lib/python3.7/site-packages (from torch==1.10.0) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0a0+37c1f4a\n",
      "    Uninstalling torch-1.8.0a0+37c1f4a:\n",
      "      Successfully uninstalled torch-1.8.0a0+37c1f4a\n",
      "Successfully installed torch-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a2050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIR = 'input'\n",
    "\n",
    "TRAIN_TEST_PATH = os.path.join(DIR, 'train_test.parquet') \n",
    "RECS_TEST_PATH = os.path.join(DIR, 'recs_test.parquet')  \n",
    "TEST_IDS_PATH = os.path.join(DIR, 'test_ids.csv') \n",
    "\n",
    "CLUSTERS_PATH = os.path.join(DIR, 'clusters.parquet')  \n",
    "USER_DECODER_PATH = os.path.join(DIR, 'user_decoder.pkl') \n",
    "RANKER_MODEL_PATH = os.path.join(DIR, 'ranker_model.pkl')\n",
    "RANKER_MODEL1_PATH = os.path.join(DIR, 'ranker_model1.pkl')\n",
    "RANKER_MODEL2_PATH = os.path.join(DIR, 'ranker_model2.pkl')\n",
    "\n",
    "MF_MODEL_PATH = os.path.join(DIR, 'mf_model.pkl')\n",
    "NN_MODEL_PATH = os.path.join(DIR, 'nn_model.pkl')\n",
    "RECS_NN_TEST_PATH = os.path.join(DIR, 'recs_nn_test.parquet')\n",
    "\n",
    "TOPK_TEST_PATH = os.path.join(DIR, 'topk_test.parquet')\n",
    "TOPK_TEST1_PATH = os.path.join(DIR, 'topk_test1.parquet')\n",
    "TOPK_TEST2_PATH = os.path.join(DIR, 'topk_test2.parquet')\n",
    "\n",
    "NUM_CLUSTERS = 8000\n",
    "NUM_USERS = 1595239\n",
    "NUM_RETAILERS = 118\n",
    "NUM_CITIES = 148\n",
    "\n",
    "SUBMIT_PATH = os.path.join(DIR, 'submission.csv')  # 'output/submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb161dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_CLUSTERS = [ \n",
    "    937, 6849, 4873, 7052,  789, 4938, 5998, 5124, 4083,  345,  721,\n",
    "    4018, 6995, 3334, 4327, 7401, 3684,  292, 7454, 5452, 1023, 6674,\n",
    "    3366, 4236, 6983, 4647, 2214, 2895, 3205, 4031, 2578,   42, 7855,\n",
    "    931, 3107, 2000, 7532, 6761, 1131, 3717, 2351, 2728, 4929, 3027,\n",
    "    612,   21, 1902,  807, 4001, 3771, 1705,  602, 1020, 6428, 6699,\n",
    "    6271,  554, 4308, 7589, 7002, 1997,  696,  595, 6675, 1751,  923,\n",
    "    6711,  999, 1666, 1263,  919, 7602, 2285, 4543, 6051, 4540, 4828,\n",
    "    3543, 6928, 1886, 6029, 5320, 2924, 7449, 4906, 7757, 1077, 5378,\n",
    "    6189, 1747, 7691, 2595,  811,  103, 7043, 1339, 1574, 2570, 1249,\n",
    "    735, 3173, 4739, 2152, 2226, 6021, 7739, 7777, 5187, 5299, 2604,\n",
    "    6569, 5893,  466, 3483, 3640, 3870, 1442, 7114, 1338, 7747, 1867,\n",
    "    2702, 3046, 1182, 1409, 4663, 4932, 1570, 6053, 6071, 3733,  712,\n",
    "    3549, 6668, 1006, 4358, 4285, 3668,  885, 4129, 3293,  407, 4392,\n",
    "    3555, 5812,  129,  163, 3018, 7752, 6998, 5949, 1266, 6656, 2786,\n",
    "    2199, 2644, 4201, 3514, 6147, 4426, 7495, 5096, 5653,  341, 1826,\n",
    "    5380,  587, 4062, 6069, 2881, 1377, 6548, 2685, 2629, 7028, 6831,\n",
    "    7181, 3251, 3948, 1357, 4438, 1138, 7528, 6149, 7514, 4835, 3938,\n",
    "    1932, 3358, 2503,   11, 1623, 4028, 1890, 6696,  354,  960, 1765,\n",
    "    3699, 7636,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3913f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.sparse as sp\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class MaxFactorDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            users: Iterable[int],\n",
    "            items: Iterable[int],\n",
    "            device: str = 'cpu',\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.users = torch.LongTensor(users)\n",
    "        self.items = torch.LongTensor(items)\n",
    "        self.num_interactions = len(users)\n",
    "        self.num_items = int(max(items) + 1)\n",
    "        \n",
    "        self.index = None\n",
    "        self.batch_size = None\n",
    "        self.neg_sample = None\n",
    "        self.num_batches = None\n",
    "        self.targets = None\n",
    "\n",
    "    def init_params(self, batch_size: int, neg_sample: int):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.neg_sample = neg_sample\n",
    "        self.num_batches = int((self.num_interactions - 1) / batch_size + 1)\n",
    "        self.targets = torch.zeros(self.batch_size, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, batch_num):\n",
    "\n",
    "        i = batch_num * self.batch_size\n",
    "        size = min(self.num_interactions - i, self.batch_size)\n",
    "\n",
    "        index = self.index[i: i + size].to(self.device)\n",
    "        items_pos = self.items[index].to(self.device)\n",
    "        users = self.users[index].to(self.device)\n",
    "\n",
    "        items_pos = items_pos.reshape(-1, 1)\n",
    "        items_neg = torch.randint(high=self.num_items, size=(size, self.neg_sample), device=self.device)\n",
    "        targets = self.targets[:size].to(self.device)\n",
    "\n",
    "        return (\n",
    "            users,\n",
    "            items_pos,\n",
    "            items_neg,\n",
    "            targets,\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.index = torch.randperm(self.num_interactions)\n",
    "        for i in range(self.num_batches):\n",
    "            yield self[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "\n",
    "class MaxFactorModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_users: int,\n",
    "            num_items: int,\n",
    "            dim: int,\n",
    "            learning_rate: float,\n",
    "            device: str = 'cpu',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.negative_sampling_batch_size = None\n",
    "        self.hard_neg_sample = None\n",
    "        self.device = device\n",
    "        \n",
    "        self.item_embeddings = torch.nn.Embedding(num_items, dim).to(self.device)\n",
    "        self.user_embeddings = torch.nn.Embedding(num_users, dim).to(self.device)\n",
    "        torch.nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.user_embeddings.weight)\n",
    "        self.optimizer = torch.optim.Adagrad(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_hard_negatives(self, users, items_neg):\n",
    "\n",
    "        hard_negatives = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(users), self.negative_sampling_batch_size):\n",
    "                neg = self(\n",
    "                    users[i: i + self.negative_sampling_batch_size],\n",
    "                    items_neg[i: i + self.negative_sampling_batch_size],\n",
    "                )\n",
    "                topk = torch.topk(neg, self.hard_neg_sample)[1]\n",
    "                hard_negatives.append(items_neg[i: i + self.negative_sampling_batch_size].gather(1, topk))\n",
    "        items_neg = torch.cat(hard_negatives, dim=0)\n",
    "        return items_neg\n",
    "\n",
    "    def _fit(\n",
    "            self,\n",
    "            dataset: MaxFactorDataset,\n",
    "            epochs: int,\n",
    "            learning_rate: float,\n",
    "             penalty_alpha: float,\n",
    "    ):\n",
    "\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for users, items_pos, items_neg, targets in dataset:\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.hard_neg_sample:\n",
    "                    items_neg = self.get_hard_negatives(users, items_neg)\n",
    "                items = torch.cat([items_pos, items_neg], dim=1)\n",
    "                penalty = (((self.item_embeddings.weight ** 2).sum(1) - 1) ** 2).mean()\n",
    "                score = self(users, items) \n",
    "                loss = loss_function(score, targets) + penalty * penalty_alpha\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            dataset: MaxFactorDataset,\n",
    "            epochs: int,\n",
    "            batch_size: int,\n",
    "            neg_sample: int,\n",
    "            negative_sampling_batch_size: int = None,\n",
    "            hard_neg_sample: int = None,\n",
    "            learning_rate: float = 0.015,\n",
    "            penalty_alpha: float = 0.003,\n",
    "    ):\n",
    "        dataset.init_params(batch_size, neg_sample)\n",
    "        self.negative_sampling_batch_size = negative_sampling_batch_size\n",
    "        self.hard_neg_sample = hard_neg_sample\n",
    "\n",
    "        self._fit(dataset, epochs, learning_rate, penalty_alpha)\n",
    "\n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor) -> torch.FloatTensor:\n",
    "\n",
    "        user_embeddings = self.user_embeddings(users).unsqueeze(2)\n",
    "        item_embeddings = self.item_embeddings(items)\n",
    "        score = torch.bmm(item_embeddings, user_embeddings).squeeze(2)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, users: torch.LongTensor, items: torch.LongTensor) -> torch.FloatTensor:\n",
    "\n",
    "        user_embeddings = self.user_embeddings(users)\n",
    "        item_embeddings = self.item_embeddings(items).t()\n",
    "        score = torch.mm(user_embeddings, item_embeddings)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _create_recommendations(\n",
    "            self,\n",
    "            target_users: Iterable[int],\n",
    "            target_items: Iterable[int],\n",
    "            num_recommendations: int,\n",
    "    ):\n",
    "        target_users = torch.LongTensor(target_users).to(self.device)\n",
    "        target_items = torch.LongTensor(target_items).to(self.device)\n",
    "\n",
    "        topk = min(num_recommendations, target_items.shape[0])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res = self.predict(target_users, target_items)\n",
    "            recom = torch.topk(res, topk)\n",
    "            items = target_items[recom[1]].flatten()\n",
    "            scores = recom[0].flatten()\n",
    "            users = target_users.reshape(-1, 1).repeat(1, topk).flatten()\n",
    "\n",
    "        users = users.cpu().detach().numpy()\n",
    "        items = items.cpu().detach().numpy()\n",
    "        scores = scores.cpu().detach().numpy()\n",
    "\n",
    "        return users, items, scores\n",
    "\n",
    "    def create_recommendations(\n",
    "            self,\n",
    "            target_users: Iterable[int],\n",
    "            target_items: Iterable[int],\n",
    "            num_recommendations: int,\n",
    "    ) -> (np.array, np.array, np.array):\n",
    "        \n",
    "        num_batch_users = int(200 ** 3 / 4 / len(target_items))\n",
    "\n",
    "        all_users = []\n",
    "        all_items = []\n",
    "        all_scores = []\n",
    "\n",
    "        for i in range(0, len(target_users), num_batch_users):\n",
    "            users, items, scores = self._create_recommendations(\n",
    "                target_users[i:i + num_batch_users],\n",
    "                target_items,\n",
    "                num_recommendations,\n",
    "            )\n",
    "\n",
    "            all_users.append(users)\n",
    "            all_items.append(items)\n",
    "            all_scores.append(scores)\n",
    "\n",
    "        all_users = np.hstack(all_users)\n",
    "        all_items = np.hstack(all_items)\n",
    "        all_scores = np.hstack(all_scores)\n",
    "\n",
    "        return all_users, all_items, all_scores\n",
    "    \n",
    "    \n",
    "class MaxFactorRecommender:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.cnt = {}\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.recs = None\n",
    "        self.train_set = None\n",
    "        self.dataset = None\n",
    "        self.model = None\n",
    "        self.already_seen = None\n",
    "\n",
    "    def init_model(self):\n",
    "\n",
    "        self.dataset = MaxFactorDataset(\n",
    "            users=self.train_set['user'].values,\n",
    "            items=self.train_set['item'].values,\n",
    "            device=self.config['device'],\n",
    "        )\n",
    "\n",
    "        self.model = MaxFactorModel(\n",
    "            num_users=self.cnt['users'],\n",
    "            num_items=self.cnt['items'],\n",
    "            dim=self.config['dim'],\n",
    "            learning_rate=self.config['fit_params']['learning_rate'],\n",
    "            device=self.config['device'],\n",
    "        )\n",
    "\n",
    "    def encode_ids(self):\n",
    "        self.train_set['user'] = self.user_encoder.fit_transform(self.train_set['user_id'])\n",
    "        self.train_set['item'] = self.item_encoder.fit_transform(self.train_set['cluster_id'])\n",
    "        self.cnt['items'] = self.train_set.item.max() + 1\n",
    "        self.cnt['users'] = self.train_set.user.max() + 1\n",
    "        self.already_seen = self.get_user_item_id(\n",
    "            user_col=self.train_set['user'],\n",
    "            item_col=self.train_set['item'],\n",
    "        ).drop_duplicates().values\n",
    "\n",
    "    def decode_ids(self):\n",
    "        self.recs['user_id'] = self.user_encoder.classes_[self.recs.user]\n",
    "        self.recs['cluster_id'] = self.item_encoder.classes_[self.recs.item]\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.fit(\n",
    "            dataset=self.dataset,\n",
    "            **self.config['fit_params'],\n",
    "        )\n",
    "\n",
    "    def torch_recommend(self, users):\n",
    "        all_items = []\n",
    "        all_users = []\n",
    "        all_scores = []\n",
    "        target_users = self.user_encoder.transform(users)\n",
    "        target_items = np.arange(self.cnt['items'])\n",
    "        users, items, scores = self.model.create_recommendations(\n",
    "            target_users,\n",
    "            target_items,\n",
    "            self.config['num_recommendations'],\n",
    "        )\n",
    "        all_items.append(items.astype(np.uint16))\n",
    "        all_users.append(users.astype(np.int32))\n",
    "        all_scores.append(scores)\n",
    "\n",
    "        all_items = np.hstack(all_items)\n",
    "        all_users = np.hstack(all_users)\n",
    "        all_scores = np.hstack(all_scores)\n",
    "\n",
    "        self.recs = pd.DataFrame()\n",
    "        self.recs['user'] = all_users\n",
    "        self.recs['item'] = all_items\n",
    "        self.recs['score'] = all_scores\n",
    "       \n",
    "    @staticmethod\n",
    "    def get_user_item_id(user_col: pd.Series, item_col: pd.Series) -> pd.Series:\n",
    "        return item_col.astype(np.int64) * (10 ** 8) + user_col\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_rank(col, df):\n",
    "        if len(df) == 0:\n",
    "            return []\n",
    "        _, index, num_ranges = np.unique(df[col], return_counts=True, return_index=True)\n",
    "        num_ranges = num_ranges[index.argsort()]\n",
    "        arange = np.arange(num_ranges.max(), dtype=int)\n",
    "        ranks = np.hstack([arange[:i] for i in num_ranges])\n",
    "        return ranks\n",
    "        \n",
    "    def filter_seen_recs(self):\n",
    "        # self.recs['ui'] = self.get_user_item_id(\n",
    "        #     user_col=self.recs['user'],\n",
    "        #     item_col=self.recs['item'],\n",
    "        # )\n",
    "        # seen = self.recs.ui.isin(self.already_seen)\n",
    "        # self.recs = self.recs[~seen]\n",
    "        self.recs['rnk'] = self.apply_rank('user', self.recs)\n",
    "        \n",
    "    def create_recommendations(\n",
    "            self,\n",
    "            train_set: pd.DataFrame,\n",
    "            users: Iterable[int],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\"\n",
    "        :return\n",
    "        pd.DataFrame({\n",
    "            id: [1, 2, 3],\n",
    "            cluster_id: [4, 5, 6],\n",
    "            score: [0.1, 0.3, -0.2],\n",
    "            rnk: [0, 1, 2],\n",
    "        })\n",
    "        \"\"\"\n",
    "        self.train_set = train_set\n",
    "        self.encode_ids()\n",
    "        self.init_model()\n",
    "        self.fit()\n",
    "        self.torch_recommend(users)\n",
    "        self.filter_seen_recs()\n",
    "        self.decode_ids()\n",
    "        return self.recs[['user_id', 'cluster_id', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cccaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recs_mf():\n",
    "    test_ids = pd.read_csv(TEST_IDS_PATH)\n",
    "    user_decoder = pickle.load(open(USER_DECODER_PATH, 'rb'))\n",
    "    user_ecnoder = dict(zip(user_decoder, np.arange(len(user_decoder))))\n",
    "    test_ids['user_id'] = test_ids['id'].map(user_ecnoder)\n",
    "\n",
    "    recommender = pickle.load(open(MF_MODEL_PATH, 'rb'))\n",
    "\n",
    "    recommender.torch_recommend(test_ids['user_id'])\n",
    "    recommender.filter_seen_recs()\n",
    "    recommender.decode_ids()\n",
    "    recs_test = recommender.recs[['user_id', 'cluster_id', 'score']]\n",
    "\n",
    "    recs_test.to_parquet(RECS_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89633d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory_profiler\n",
      "  Using cached memory_profiler-0.60.0-py3-none-any.whl\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/python3.7/envs/aikit/lib/python3.7/site-packages (from memory_profiler) (5.8.0)\n",
      "Installing collected packages: memory-profiler\n",
      "Successfully installed memory-profiler-0.60.0\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e4ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc082d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.0.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 8220.55 MiB, increment: 7941.73 MiB\n",
      "CPU times: user 1min 14s, sys: 8.24 s, total: 1min 22s\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "create_recs_mf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2817a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        dim = 2 * NUM_CLUSTERS + NUM_RETAILERS + NUM_CITIES\n",
    "        self.linear = torch.nn.Linear(dim, 10000).to(self.device)\n",
    "        self.linear2 = torch.nn.Linear(10000, NUM_CLUSTERS).to(self.device)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear2(self.relu(self.linear(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e7e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, x, y, users, batch_size, device='cuda'):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.users = users\n",
    "        self.num_users = len(users)\n",
    "        self.num_batches = int((self.num_users - 1) / batch_size + 1)\n",
    "        \n",
    "    def __getitem__(self, batch_num):\n",
    "        \n",
    "        i = batch_num * self.batch_size\n",
    "        size = min(self.num_users - i, self.batch_size)\n",
    "        users = self.users[i: i + size]\n",
    "        if self.y is not None:\n",
    "            return (torch.FloatTensor(self.x[users].todense()).to(self.device), \n",
    "                  torch.FloatTensor(self.y[users].todense()).to(self.device))\n",
    "        else:\n",
    "            return torch.FloatTensor(self.x[users].todense()).to(self.device), None\n",
    "            \n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.users)\n",
    "        for i in range(self.num_batches):\n",
    "            yield self[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa7d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(short_train, col, num_classes, use_ones=False):\n",
    "    df = short_train[['user_id', col]].drop_duplicates()\n",
    "    df[f'user_{col}'] = df['user_id'].astype(np.int64) * 10000 + df[col]\n",
    "    df['user_col_count'] = df[f'user_{col}'].map(short_train[f'user_{col}'].value_counts()) \n",
    "    df['user_count'] = df['user_id'].map(short_train['user_id'].value_counts()) \n",
    "    df['user_col_share'] = df['user_col_count'] / df['user_count']\n",
    "    if use_ones:\n",
    "        return sp.csr_matrix((np.ones(len(df)), (df['user_id'], df[col])), shape=(NUM_USERS, num_classes))\n",
    "    return sp.csr_matrix((df['user_col_share'], (df['user_id'], df[col])), shape=(NUM_USERS, num_classes))\n",
    "\n",
    "def create_x_y(train_val, val=None):\n",
    "    short_train = train_val[~train_val[['order_id', 'cluster_id']].duplicated()]\n",
    "    short_train['user_retailer_id'] = short_train['user_id'].astype(np.int64) * 10000 + short_train['retailer_id']\n",
    "    short_train['user_city_id'] = short_train['user_id'].astype(np.int64) * 10000 + short_train['city_id']\n",
    "    short_train['user_cluster_id'] = short_train['user_id'].astype(np.int64) * 10000 + short_train['cluster_id']\n",
    "\n",
    "    x1 = create_sparse_matrix(short_train, 'retailer_id', NUM_RETAILERS)\n",
    "    x2 = create_sparse_matrix(short_train, 'city_id', NUM_CITIES)\n",
    "    x3 = create_sparse_matrix(short_train, 'cluster_id', NUM_CLUSTERS)\n",
    "    x4 = create_sparse_matrix(short_train, 'cluster_id', NUM_CLUSTERS, True)\n",
    "\n",
    "    x = sp.hstack([x1, x2, x3, x4], format='csr')\n",
    "    if val is not None:\n",
    "        y = sp.csr_matrix((np.ones(len(val)), [val['user_id'], val['cluster_id']]), shape=(NUM_USERS, NUM_CLUSTERS))\n",
    "        return x, y\n",
    "    else:\n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618733ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec(model, dataset, topk=160):\n",
    "    items = []\n",
    "    scores = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataset:\n",
    "            score = model(x)\n",
    "            recom = torch.topk(score, topk)\n",
    "            items.append(recom[1].flatten().cpu().detach().numpy().astype(np.int16))\n",
    "            scores.append(recom[0].flatten().cpu().detach().numpy())\n",
    "\n",
    "    users = dataset.users.reshape(-1, 1).repeat(topk, 1).flatten()\n",
    "    items = np.hstack(items)\n",
    "    scores = np.hstack(scores)\n",
    "\n",
    "    recs = pd.DataFrame()\n",
    "    recs['user_id'] = users\n",
    "    recs['cluster_id'] = items\n",
    "    recs['scores'] = scores\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4022d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recs_nn():\n",
    "    config = {\n",
    "        'batch_size': 3000,\n",
    "        'device': 'cpu',\n",
    "    }\n",
    "    model = pickle.load(open(NN_MODEL_PATH, 'rb'))\n",
    "    train_test =  pd.read_parquet(TRAIN_TEST_PATH)\n",
    "    test_ids = pd.read_csv(TEST_IDS_PATH)\n",
    "    user_decoder = pickle.load(open(USER_DECODER_PATH, 'rb'))\n",
    "    user_ecnoder = dict(zip(user_decoder, np.arange(len(user_decoder))))\n",
    "    test_ids['user_id'] = test_ids['id'].map(user_ecnoder)\n",
    "    \n",
    "    x, y = create_x_y(train_test)\n",
    "    dataset = Dataset(x, y, np.array(test_ids['user_id']).astype(np.int32), \n",
    "                      config['batch_size'], config['device'])\n",
    "    recs = get_rec(model, dataset)\n",
    "    recs.to_parquet(RECS_NN_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c2ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/intel/oneapi/intelpython/latest/envs/aikit/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "create_recs_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def most_common(array):\n",
    "    elements, counts = np.unique(array, return_counts=True)\n",
    "    return elements[np.argpartition(counts, kth=-1)[-1]]\n",
    "\n",
    "def apply_rank(col, df):\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    _, index, num_ranges = np.unique(df[col], return_counts=True, return_index=True)\n",
    "    num_ranges = num_ranges[index.argsort()]\n",
    "    arange = np.arange(num_ranges.max(), dtype=int)\n",
    "    ranks = np.hstack([arange[:i] for i in num_ranges])\n",
    "    return ranks\n",
    "\n",
    "def get_mean_diff_dt(array):\n",
    "    if len(array) == 1:\n",
    "        return -1\n",
    "    np_array = np.array(array)\n",
    "    np_array[1:] - np_array[:-1]\n",
    "    return (np_array[1:] - np_array[:-1]).mean()\n",
    "    \n",
    "def create_features_simple(table, train, users, clusters):\n",
    "    table['count_item_id'] = (table.cluster_id.map(train['cluster_id'].value_counts()).fillna(0) / len(train)).astype(np.float32)\n",
    "\n",
    "    table['num_orders'] = table['user_id'].map(\n",
    "        train[['order_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    table['num_order_with_target_item'] = table['ui'].map(\n",
    "        train[['order_id', 'ui']].drop_duplicates()['ui'].value_counts()\n",
    "    ).fillna(0).astype(np.int16)\n",
    "    \n",
    "    last_order_ui = train[train.dt == \\\n",
    "          train['user_id'].map(\n",
    "                train[['user_id', 'dt']].drop_duplicates().groupby('user_id').max()['dt']\n",
    "    )].ui.unique()\n",
    "\n",
    "    table['was_in_last_order'] = table['ui'].isin(last_order_ui).astype(np.int8)\n",
    "    del last_order_ui\n",
    "\n",
    "    prod_quantity = train.groupby('ui')['product_quantity'].sum()\n",
    "    table['prod_quantity'] = table['ui'].map(prod_quantity).fillna(0).astype(np.int16)\n",
    "    del prod_quantity\n",
    "    \n",
    "    prev_order_ui = train['dt'].max() - train.groupby('ui')['dt'].max()\n",
    "    table['prev_order_ui'] = table['ui'].map(prev_order_ui).fillna(-1).astype(np.float32)\n",
    "    del prev_order_ui\n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id']].duplicated()\n",
    "    \n",
    "    table['user_retailer_most_common'] = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').retailer_id.apply(most_common)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    user_city_most_common = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').city_id.apply(most_common)\n",
    "    ).astype(np.int16)\n",
    "    \n",
    "    del mask\n",
    "    \n",
    "    item_city_vc = (train['cluster_id'] * 100 + train['city_id']).value_counts()\n",
    "    item_user_city = table['cluster_id'] * 100 + user_city_most_common\n",
    "    table['user_item_city_vc'] = item_user_city.map(item_city_vc).fillna(0).astype(np.float32)\n",
    "    del item_city_vc\n",
    "    del item_user_city\n",
    "    \n",
    "    for col in ['cluster_size', 'd_mean', 'd_median']:\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        \n",
    "    short_train = train[train.user_id.isin(users)]\n",
    "    \n",
    "    table['product_quantity_sum'] = table.user_id.map(\n",
    "          short_train.groupby('user_id').product_quantity.sum()\n",
    "    )\n",
    "    table['user_retailer_num'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').retailer_id.nunique()\n",
    "    ).astype(np.int8)\n",
    "    table['user_city_num'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').city_id.nunique()\n",
    "    ).astype(np.int8)\n",
    "    table['user_product_price_mean'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').product_price.mean()\n",
    "    )\n",
    "    table['user_product_discount_mean'] = table.user_id.map(\n",
    "        (short_train.product_discount != 0).groupby(short_train.user_id).mean()\n",
    "    ).astype(np.float16)\n",
    "    table['user_num_clusters'] = table['user_id'].map(\n",
    "        short_train[['cluster_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "    table['last_user_city_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').city_id.last()\n",
    "    )\n",
    "    table['last_user_retailer_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').retailer_id.last()\n",
    "    )\n",
    "    \n",
    "    table['user_most_common_cluster_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').cluster_id.apply(most_common)\n",
    "    )\n",
    "    del short_train\n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id', 'cluster_id']].duplicated()\n",
    "\n",
    "    table['cluster_quantity_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_quantity.mean().astype(np.float16)\n",
    "    )\n",
    "\n",
    "    table['cluster_city_count'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').city_id.nunique()\n",
    "    ).astype(np.float16)\n",
    "    \n",
    "    table['cluster_num_stores'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').store_id.nunique()\n",
    "    ).astype(np.float16)\n",
    "    del mask\n",
    "\n",
    "    table['cluster_product_price_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_price.mean()\n",
    "    ).astype(np.float16)\n",
    "\n",
    "    table['cluster_mean_discount'] = table['cluster_id'].map(\n",
    "        (train.product_discount == 0).groupby(train.cluster_id).mean().astype(np.float16)\n",
    "    )\n",
    "\n",
    "    table['num_users_bought_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').user_id.nunique()\n",
    "    ).fillna(0).astype(np.float16)\n",
    "\n",
    "    table['num_orders_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').order_id.nunique()\n",
    "    ).fillna(0).astype(np.float16)\n",
    "    \n",
    "    mask = ~train[['order_id', 'cluster_id']].duplicated()\n",
    "    short_train = train[mask]\n",
    "\n",
    "    city_retailer = short_train.city_id.astype(np.int16) * 100 + short_train.retailer_id\n",
    "    city_retailer_cluster = city_retailer.astype(np.int64) * 10000 + short_train.cluster_id\n",
    "\n",
    "    city_retailer_user = user_city_most_common.astype(np.int16) * 100 + \\\n",
    "        table['user_retailer_most_common']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    table['f1'] = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f2'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f3'] = table['f2'] \\\n",
    "        / table['f1'] \n",
    "    \n",
    "    \n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "\n",
    "    city_retailer_user = table['last_user_city_id'].astype(np.int16) * 100 + \\\n",
    "        table['last_user_retailer_id']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    f4 = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f5'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f6'] = table['f5'] \\\n",
    "        / f4 \n",
    "    del f4\n",
    "    \n",
    "    del city_retailer\n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "    del city_retailer_cluster\n",
    "\n",
    "    ui_vc = train.ui.value_counts()\n",
    "    rnk_vc = train[['user_id', 'ui', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['vc'] = rnk_vc.ui.map(ui_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['user_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_user_id_ui'] = apply_rank('user_id', rnk_vc)\n",
    "    table['rnk_user_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_user_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del ui_vc\n",
    "\n",
    "    rnk_vc = rnk_vc.sort_values(['cluster_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_id_ui'] = apply_rank('cluster_id', rnk_vc)\n",
    "    table['rnk_cluster_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_cluster_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    rnk_vc = train['cluster_id'].value_counts().to_frame()\n",
    "    rnk_vc['rnk_cluster_id'] = np.arange(len(rnk_vc))\n",
    "    table['rnk_cluster_id'] = table.cluster_id.map(rnk_vc['rnk_cluster_id']\n",
    "                                                  ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    cluster_city_vc = (train['city_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['city_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_city'] = rnk_vc['city_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_city'].map(cluster_city_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['city_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_city'] = apply_rank('city_id', rnk_vc)\n",
    "    user_city_cluster = table['last_user_city_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_city'] = user_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_city')['rnk_cluster_city']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_city_vc\n",
    "    del rnk_vc\n",
    "    del user_city_cluster\n",
    "\n",
    "    cluster_retailer_vc = (train['retailer_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer'] = rnk_vc['retailer_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer'].map(cluster_retailer_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_retailer'] = apply_rank('retailer_id', rnk_vc)\n",
    "    user_retailer_cluster = table['last_user_retailer_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_cluster = table['user_retailer_most_common'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer2'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_cluster\n",
    "\n",
    "    cluster_retailer_city_vc = (train['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        train['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        train['cluster_id']).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id', 'city_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        rnk_vc['cluster_id'])\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer_city'].map(cluster_retailer_city_vc)\n",
    "    rnk_vc['retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 1000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64))\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_city', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluser_city_retailer'] = apply_rank('retailer_city', rnk_vc)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['last_user_retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['user_retailer_most_common'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city2'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_city_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_city_cluster\n",
    "\n",
    "    \n",
    "    return table\n",
    "\n",
    "def create_table(train, recs_nn, recs_mf, users):\n",
    "    \n",
    "    recs_nn['rnk'] = apply_rank('user_id', recs_nn)\n",
    "    recs_mf['rnk'] = apply_rank('user_id', recs_mf)\n",
    "\n",
    "    mask1 = recs_nn['user_id'].isin(users)\n",
    "    mask2 = ~recs_mf.ui.isin(recs_nn.ui) & recs_mf['user_id'].isin(users)\n",
    "    mask3 = ~(train.ui.isin(recs_nn.ui) | train.ui.isin(recs_mf.ui) \\\n",
    "              | train.ui.duplicated())  & train['user_id'].isin(users)\n",
    "    \n",
    "    table = pd.concat([\n",
    "        recs_nn[['user_id', 'cluster_id']][mask1], \n",
    "        recs_mf[['user_id', 'cluster_id']][mask2], \n",
    "        train[['user_id', 'cluster_id']][mask3]\n",
    "    ])\n",
    "    table.reset_index(drop=True, inplace=True)\n",
    "    del mask1\n",
    "    del mask2\n",
    "    del mask3\n",
    "    table['ui'] = table['user_id'].astype(np.int64) * 10000 + table['cluster_id']\n",
    "    \n",
    "    table['rnk'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['scores']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    recs_nn = recs_nn[~recs_nn.ui.isin(train.ui)]\n",
    "    recs_nn['rnk2'] = apply_rank('user_id', recs_nn)\n",
    "    table['rnk2'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['rnk3'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score2'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['score']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    recs_mf = recs_mf[~recs_mf.ui.isin(train.ui)]\n",
    "    recs_mf['rnk2'] = apply_rank('user_id', recs_mf)\n",
    "    table['rnk4'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['rnk2']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    return table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs(pred, users, items, already_bought, weights, num_recs=20):\n",
    "    fix_pred = pred * (1.37 - already_bought) * (weights ** 1.5)\n",
    "    indexes = (-fix_pred).argsort()\n",
    "    recs = defaultdict(list)\n",
    "    for user_id, item_id in zip(users[indexes], items[indexes]):\n",
    "        if len(recs[user_id]) < num_recs:\n",
    "            recs[user_id].append(item_id)\n",
    "    return recs\n",
    "\n",
    "\n",
    "def get_cluster_weights(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    cluster_popularity = dataset[\"cluster_id\"].value_counts().sort_values(ascending=True).reset_index()\n",
    "    cluster_popularity.columns = [\"cluster_id\", \"cnt\"]\n",
    "    cluster_popularity[\"rank\"] = cluster_popularity[\"cnt\"].rank(method=\"dense\") + 1\n",
    "    cluster_popularity[\"w\"] = 1 / np.log10(cluster_popularity[\"rank\"])\n",
    "\n",
    "    return cluster_popularity[[\"cluster_id\", \"w\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(train_path, recs_nn_path, recs_mf_path, \n",
    "              users, create_features_func, val_path=None):\n",
    "\n",
    "    train = pd.read_parquet(train_path)\n",
    "    train['product_price'] = train['product_price'].astype(np.float16)\n",
    "    train['product_discount'] = train['product_discount'].astype(np.float16)\n",
    "\n",
    "    recs_nn = pd.read_parquet(recs_nn_path)\n",
    "    recs_mf = pd.read_parquet(recs_mf_path)\n",
    "    clusters = pd.read_parquet(CLUSTERS_PATH)\n",
    "    \n",
    "    for df in [train, recs_nn, recs_mf]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "    \n",
    "    table = create_table(train, recs_nn, recs_mf, users)\n",
    "    del recs_nn\n",
    "    del recs_mf\n",
    "\n",
    "    table = create_features_func(table, train, users, clusters)\n",
    "    del train\n",
    "    del clusters\n",
    "\n",
    "    X = table.drop(['user_id', 'ui'], axis=1).to_numpy(dtype=np.float32)\n",
    "\n",
    "    if val_path is None:\n",
    "        return X\n",
    "    \n",
    "    val = pd.read_parquet(val_path)\n",
    "    val['ui'] = val['user_id'].astype(np.int64) * 10000 + val['cluster_id']\n",
    "    y = np.array(table['ui'].isin(val['ui']))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_data(train_path, recs_nn_path, recs_mf_path, users):\n",
    "    \n",
    "    train = pd.read_parquet(train_path)\n",
    "    recs_nn = pd.read_parquet(recs_nn_path)\n",
    "    recs_mf = pd.read_parquet(recs_mf_path)\n",
    "    clusters = pd.read_parquet(CLUSTERS_PATH)\n",
    "    \n",
    "    for df in [train, recs_nn, recs_mf]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "    \n",
    "    table = create_table(train, recs_nn, recs_mf, users)\n",
    "    \n",
    "    already_bought = np.array(table['ui'].isin(train['ui']))\n",
    "    cluster_weights = get_cluster_weights(train)\n",
    "    weights = np.array(table.cluster_id.map(\n",
    "        cluster_weights.set_index('cluster_id')['w']\n",
    "    ).fillna(cluster_weights['w'].max()))\n",
    "    del cluster_weights\n",
    "    \n",
    "    return (\n",
    "        np.array(table['user_id']), \n",
    "        np.array(table['cluster_id']), \n",
    "        already_bought, \n",
    "        weights\n",
    "    )\n",
    "\n",
    "def _create_top_k(train_path, recs_nn_path, recs_mf_path, \n",
    "                 users, model_path, top_k_path, model_path2=None, k=120):\n",
    "    \n",
    "    X = get_table(train_path, recs_nn_path, recs_mf_path, users, create_features_simple)\n",
    "    print(0)\n",
    "    if model_path2 is None:\n",
    "        ranker_model = pickle.load(open(model_path, 'rb'))\n",
    "        pred = ranker_model.predict(X)\n",
    "    else:\n",
    "        ranker_model = pickle.load(open(model_path, 'rb'))\n",
    "        pred1 = ranker_model.predict(X)\n",
    "        ranker_model = pickle.load(open(model_path2, 'rb'))\n",
    "        pred2 = ranker_model.predict(X)\n",
    "        pred = np.mean([pred1, pred2], axis=0)\n",
    "        \n",
    "    del X\n",
    "    print(1)\n",
    "    users, items, already_bought, weights = get_some_data(\n",
    "        train_path, recs_nn_path, recs_mf_path, users\n",
    "    )\n",
    "    recs = get_recs(pred, users, items, already_bought, weights, num_recs=k)\n",
    "    \n",
    "    users = []\n",
    "    items = []\n",
    "    for user_id in recs:\n",
    "        users += [user_id] * len(recs[user_id])\n",
    "        items += recs[user_id]\n",
    "    del recs\n",
    "\n",
    "    top_k = pd.DataFrame()\n",
    "    top_k['user_id'] = users\n",
    "    top_k['cluster_id'] = items\n",
    "    top_k.to_parquet(top_k_path)\n",
    "\n",
    "def create_top_k():\n",
    "    \n",
    "    test_ids = pd.read_csv(TEST_IDS_PATH)\n",
    "    user_decoder = pickle.load(open(USER_DECODER_PATH, 'rb'))\n",
    "    user_ecnoder = dict(zip(user_decoder, np.arange(len(user_decoder))))\n",
    "    users = test_ids['id'].map(user_ecnoder)\n",
    "    users1 = users[users % 2 == 0]\n",
    "    users2 = users[users % 2 == 1]\n",
    "\n",
    "    _create_top_k(TRAIN_TEST_PATH, RECS_NN_TEST_PATH, RECS_TEST_PATH,\n",
    "                 users1, RANKER_MODEL1_PATH, TOPK_TEST1_PATH, \n",
    "                 RANKER_MODEL2_PATH)\n",
    "    _create_top_k(TRAIN_TEST_PATH, RECS_NN_TEST_PATH, RECS_TEST_PATH,\n",
    "              users2, RANKER_MODEL1_PATH, TOPK_TEST2_PATH, \n",
    "              RANKER_MODEL2_PATH)\n",
    "    \n",
    "    topk_test1 = pd.read_parquet(TOPK_TEST1_PATH)\n",
    "    topk_test2 = pd.read_parquet(TOPK_TEST2_PATH)\n",
    "    topk_test = pd.concat([topk_test1, topk_test2])\n",
    "    topk_test.to_parquet(TOPK_TEST_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb164875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "create_top_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_table(train, table, clusters, recs_nn_path, recs_mf_path):\n",
    "    \n",
    "    users = table.user_id.unique()\n",
    "    \n",
    "    recs_nn = pd.read_parquet(recs_nn_path)\n",
    "    recs_mf = pd.read_parquet(recs_mf_path)\n",
    "    \n",
    "    for df in [train, recs_nn, recs_mf]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "        \n",
    "    recs_nn['rnk'] = apply_rank('user_id', recs_nn)\n",
    "    recs_mf['rnk'] = apply_rank('user_id', recs_mf)\n",
    "    \n",
    "    table['rnk'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score'] = table['ui'].map(\n",
    "        recs_nn.set_index('ui')['scores']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    mask = recs_nn.ui.isin(train.ui)\n",
    "    \n",
    "    recs_short = recs_nn[~mask]\n",
    "    recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "    table['rnk2'] = table['ui'].map(\n",
    "        recs_short.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    recs_short = recs_nn[mask]\n",
    "    recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "    table['rnk3'] = table['ui'].map(\n",
    "        recs_short.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del recs_nn\n",
    "    \n",
    "    table['rnk4'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    table['score2'] = table['ui'].map(\n",
    "        recs_mf.set_index('ui')['score']\n",
    "    ).fillna(-100).astype(np.float32)\n",
    "    \n",
    "    mask = recs_mf.ui.isin(train.ui)\n",
    "    \n",
    "    recs_short = recs_mf[~mask]\n",
    "    recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "    table['rnk5'] = table['ui'].map(\n",
    "        recs_short.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    \n",
    "    recs_short = recs_mf[mask]\n",
    "    recs_short['rnk'] = apply_rank('user_id', recs_short)\n",
    "    table['rnk6'] = table['ui'].map(\n",
    "        recs_short.set_index('ui')['rnk']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del recs_mf\n",
    "    \n",
    "#     count_user_id = table.user_id.map(train['user_id'].value_counts()).fillna(0).astype(np.int16)\n",
    "    table['count_item_id'] = (table.cluster_id.map(train['cluster_id'].value_counts()).fillna(0) / len(train)).astype(np.float32)\n",
    "\n",
    "    table['num_orders'] = table['user_id'].map(\n",
    "        train[['order_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    table['num_order_with_target_item'] = table['ui'].map(\n",
    "        train[['order_id', 'ui']].drop_duplicates()['ui'].value_counts()\n",
    "    ).fillna(0).astype(np.int16)\n",
    "    \n",
    "    last_order_ui = train[train.dt == \\\n",
    "          train['user_id'].map(\n",
    "                train[['user_id', 'dt']].drop_duplicates().groupby('user_id').max()['dt']\n",
    "    )].ui.unique()\n",
    "\n",
    "    table['was_in_last_order'] = table['ui'].isin(last_order_ui).astype(np.int8)\n",
    "    del last_order_ui\n",
    "\n",
    "    prod_quantity = train.groupby('ui')['product_quantity'].sum()\n",
    "    table['prod_quantity'] = table['ui'].map(prod_quantity).fillna(0).astype(np.int16)\n",
    "    del prod_quantity\n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id']].duplicated()\n",
    "    \n",
    "    table['user_retailer_most_common'] = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').retailer_id.apply(most_common)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    user_city_most_common = table['user_id'].map(\n",
    "        train[mask].groupby('user_id').city_id.apply(most_common)\n",
    "    ).astype(np.int16)\n",
    "    \n",
    "    del mask\n",
    "    \n",
    "#     item_retailer_vc = (train['cluster_id'] * 100 + train['retailer_id']).value_counts()\n",
    "#     item_user_retailer = table['cluster_id'] * 100 + table['user_retailer_most_common']\n",
    "#     table['user_item_retailer_vc'] = item_user_retailer.map(item_retailer_vc).fillna(0).astype(np.float32)\n",
    "#     del item_retailer_vc\n",
    "#     del item_user_retailer\n",
    "    \n",
    "    item_city_vc = (train['cluster_id'] * 100 + train['city_id']).value_counts()\n",
    "    item_user_city = table['cluster_id'] * 100 + user_city_most_common\n",
    "    table['user_item_city_vc'] = item_user_city.map(item_city_vc).fillna(0).astype(np.float32)\n",
    "    del item_city_vc\n",
    "    del item_user_city\n",
    "    \n",
    "    for col in ['cluster_size', 'd_mean', 'd_median']:\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        table['cluster_' + col] = table['cluster_id'].map(\n",
    "            clusters.set_index('cluster_id')[col]\n",
    "        )\n",
    "        \n",
    "#     # user features \n",
    "    short_train = train[train.user_id.isin(users)]\n",
    "    \n",
    "    ui_dt = defaultdict(list)\n",
    "    short_train3 = short_train[~short_train[['ui', 'order_id']].duplicated()]\n",
    "    for ui, dt in zip(short_train3['ui'], short_train3['dt']):\n",
    "        ui_dt[ui].append(dt)\n",
    "    del short_train3\n",
    "    table['ui_dt_diff_mean'] = table.ui.map(\n",
    "        {key: get_mean_diff_dt(value) for key, value in ui_dt.items()}\n",
    "    ).fillna(-1).astype(np.float32)\n",
    "    del ui_dt\n",
    "    \n",
    "    table['product_quantity_sum'] = table.user_id.map(\n",
    "          short_train.groupby('user_id').product_quantity.sum()\n",
    "    )\n",
    "    table['user_retailer_num'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').retailer_id.nunique()\n",
    "    ).astype(np.int8)\n",
    "#     table['user_city_num'] = table.user_id.map(\n",
    "#         short_train.groupby('user_id').city_id.nunique()\n",
    "#     ).astype(np.int8)\n",
    "    table['user_product_price_mean'] = table.user_id.map(\n",
    "        short_train.groupby('user_id').product_price.mean()\n",
    "    )\n",
    "#     table['user_product_price_sum'] = table.user_id.map(\n",
    "#         short_train.product_price.astype(np.float32).groupby(short_train.user_id).sum()\n",
    "#     )\n",
    "    table['user_product_discount_mean'] = table.user_id.map(\n",
    "        (short_train.product_discount != 0).groupby(short_train.user_id).mean()\n",
    "    ).astype(np.float32)\n",
    "    table['user_num_clusters'] = table['user_id'].map(\n",
    "        short_train[['cluster_id', 'user_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "    table['last_user_city_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').city_id.last()\n",
    "    )\n",
    "    table['last_user_retailer_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').retailer_id.last()\n",
    "    )\n",
    "#     table['user_mean_clusters_in_order'] = table['user_id'].map(\n",
    "#         short_train.groupby(['user_id', 'order_id']).cluster_id.nunique().reset_index() \\\n",
    "#         .groupby('user_id').cluster_id.mean()\n",
    "#     ).astype(np.float16)\n",
    "    table['user_most_common_cluster_id'] = table['user_id'].map(\n",
    "        short_train.groupby('user_id').cluster_id.apply(most_common)\n",
    "    )\n",
    "    del short_train\n",
    "    \n",
    "    # item features \n",
    "    \n",
    "    mask = ~train[['user_id', 'order_id', 'cluster_id']].duplicated()\n",
    "    \n",
    "#     table['cluster_quantity_sum'] = table['cluster_id'].map(\n",
    "#         train.groupby('cluster_id').product_quantity.sum().astype(np.float32)\n",
    "#     )\n",
    "    table['cluster_quantity_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_quantity.mean().astype(np.float32)\n",
    "    )\n",
    "\n",
    "    for retailer_id in [0, 1, 7]: # [1, 7, 0, 16, 6, 4, 19, 12, 15]\n",
    "        table[f'cluster_retailer_{retailer_id}'] = table['cluster_id'].map(\n",
    "            (train[mask].retailer_id == retailer_id).groupby(train[mask].cluster_id).mean(\n",
    "            ).astype(np.float32)\n",
    "        )\n",
    "    \n",
    "    table['cluster_city_count'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').city_id.nunique()\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "\n",
    "#     table['last_dt_delta'] = table['cluster_id'].map(\n",
    "#         train.dt.max() - train.groupby('cluster_id').dt.max()\n",
    "#     ).astype(np.float32)\n",
    "\n",
    "    table['cluster_num_stores'] = table['cluster_id'].map(\n",
    "        train[mask].groupby('cluster_id').store_id.nunique()\n",
    "    ).astype(np.float32)\n",
    "    del mask\n",
    "\n",
    "    table['cluster_product_price_mean'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').product_price.mean()\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    table['cluster_mean_discount'] = table['cluster_id'].map(\n",
    "        (train.product_discount == 0).groupby(train.cluster_id).mean().astype(np.float32)\n",
    "    )\n",
    "\n",
    "    table['num_users_bought_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').user_id.nunique()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['num_orders_cluster'] = table['cluster_id'].map(\n",
    "        train.groupby('cluster_id').order_id.nunique()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "    \n",
    "#     more features\n",
    "    \n",
    "    mask = ~train[['order_id', 'cluster_id']].duplicated()\n",
    "    short_train = train[mask]\n",
    "\n",
    "    city_retailer = short_train.city_id.astype(np.int16) * 100 + short_train.retailer_id\n",
    "    city_retailer_cluster = city_retailer.astype(np.int64) * 10000 + short_train.cluster_id\n",
    "\n",
    "    city_retailer_user = user_city_most_common.astype(np.int16) * 100 + \\\n",
    "        table['user_retailer_most_common']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    table['f1'] = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f2'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f3'] = table['f2'] \\\n",
    "        / table['f1'] \n",
    "    \n",
    "    \n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "\n",
    "    city_retailer_user = table['last_user_city_id'].astype(np.int16) * 100 + \\\n",
    "        table['last_user_retailer_id']\n",
    "    city_retailer_cluster_user = city_retailer_user.astype(np.int64)*10000 + table.cluster_id\n",
    "\n",
    "    f4 = city_retailer_user.map(\n",
    "        city_retailer.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f5'] = city_retailer_cluster_user.map(\n",
    "        city_retailer_cluster.value_counts()\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "    table['f6'] = table['f5'] \\\n",
    "        / f4 \n",
    "    del f4\n",
    "    \n",
    "    del city_retailer\n",
    "    del city_retailer_user\n",
    "    del city_retailer_cluster_user\n",
    "    del city_retailer_cluster\n",
    "    \n",
    "    #more and more features\n",
    "    \n",
    "    short_train = train[train.user_id.isin(users)]\n",
    "    short_train2 = short_train[~short_train[['user_id', 'order_id']].duplicated()]\n",
    "    \n",
    "    table['time_from_order_with_target_item'] = table.ui.map(\n",
    "        short_train.dt.max() - short_train.groupby('ui').dt.last()\n",
    "    ).fillna(-1).astype(np.float32)\n",
    "    \n",
    "    user_dt = defaultdict(list)\n",
    "    for user_id, dt in zip(short_train2['user_id'], short_train2['dt']):\n",
    "        user_dt[user_id].append(dt)\n",
    "    del short_train2\n",
    "    \n",
    "    table['user_dt_diff_mean'] = table.user_id.map(\n",
    "        {key: get_mean_diff_dt(value) for key, value in user_dt.items()}\n",
    "    ).fillna(-1).astype(np.float32)\n",
    "    del user_dt\n",
    "    \n",
    "    table['share_order_with_target_item'] = (\n",
    "        table['num_order_with_target_item'] / table['num_orders'] \n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    table['ui_num'] = table.ui.map(short_train.ui.value_counts()).fillna(0).astype(np.int16)\n",
    "#     table['share_clusters_with_target_item'] = (\n",
    "#         table['ui_num']/ table['count_user_id']\n",
    "#     ).astype(np.float32)\n",
    "\n",
    "    table['share_quatity'] = (\n",
    "        table['prod_quantity'] / table['product_quantity_sum']\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    short_train4 = short_train[\n",
    "        short_train.user_id.map(short_train.groupby('user_id').retailer_id.last()) == \\\n",
    "        short_train.retailer_id\n",
    "    ]\n",
    "        \n",
    "    table['num_order_with_last_retailer'] = table['user_id'].map(\n",
    "        short_train4[['user_id', 'order_id']].drop_duplicates()['user_id'].value_counts()\n",
    "    ).astype(np.int16)\n",
    "\n",
    "    table['num_order_with_target_item_last_retailer'] = table['ui'].map(\n",
    "        short_train4[['order_id', 'ui']].drop_duplicates()['ui'].value_counts()\n",
    "    ).fillna(0).astype(np.int16)\n",
    "    del short_train4\n",
    "\n",
    "    table['share_order_with_target_item_last_retailer'] = (\n",
    "        table['num_order_with_target_item_last_retailer'] / table['num_order_with_last_retailer']\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ui_vc = train.ui.value_counts()\n",
    "    rnk_vc = train[['user_id', 'ui', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['vc'] = rnk_vc.ui.map(ui_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['user_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_user_id_ui'] = apply_rank('user_id', rnk_vc)\n",
    "    table['rnk_user_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_user_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del ui_vc\n",
    "\n",
    "    rnk_vc = rnk_vc.sort_values(['cluster_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_id_ui'] = apply_rank('cluster_id', rnk_vc)\n",
    "    table['rnk_cluster_id_ui'] = table.ui.map(rnk_vc.set_index('ui')['rnk_cluster_id_ui']\n",
    "                                          ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    rnk_vc = train['cluster_id'].value_counts().to_frame()\n",
    "    rnk_vc['rnk_cluster_id'] = np.arange(len(rnk_vc))\n",
    "    table['rnk_cluster_id'] = table.cluster_id.map(rnk_vc['rnk_cluster_id']\n",
    "                                                  ).fillna(10000).astype(np.int16)\n",
    "    del rnk_vc\n",
    "\n",
    "    cluster_city_vc = (train['city_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['city_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_city'] = rnk_vc['city_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_city'].map(cluster_city_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['city_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_city'] = apply_rank('city_id', rnk_vc)\n",
    "    user_city_cluster = table['last_user_city_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_city'] = user_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_city')['rnk_cluster_city']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_city_vc\n",
    "    del rnk_vc\n",
    "    del user_city_cluster\n",
    "\n",
    "    cluster_retailer_vc = (train['retailer_id'].astype(np.int32) * 10000 + train['cluster_id']\n",
    "                      ).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer'] = rnk_vc['retailer_id'].astype(np.int32) * 10000 + rnk_vc['cluster_id']\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer'].map(cluster_retailer_vc)\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_id', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluster_retailer'] = apply_rank('retailer_id', rnk_vc)\n",
    "    user_retailer_cluster = table['last_user_retailer_id'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_cluster = table['user_retailer_most_common'].astype(np.int32) * 10000 \\\n",
    "        + table['cluster_id']\n",
    "    table['rnk_cluster_retailer2'] = user_retailer_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer')['rnk_cluster_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_cluster\n",
    "\n",
    "    cluster_retailer_city_vc = (train['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        train['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        train['cluster_id']).value_counts()\n",
    "    rnk_vc = train[['retailer_id', 'cluster_id', 'city_id']].drop_duplicates()\n",
    "    rnk_vc['cluster_retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 10000000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        rnk_vc['cluster_id'])\n",
    "    rnk_vc['vc'] = rnk_vc['cluster_retailer_city'].map(cluster_retailer_city_vc)\n",
    "    rnk_vc['retailer_city'] = (rnk_vc['city_id'].astype(np.int64) * 1000 + \\\n",
    "        rnk_vc['retailer_id'].astype(np.int64))\n",
    "    rnk_vc = rnk_vc.sort_values(['retailer_city', 'vc'], ascending=False)\n",
    "    rnk_vc['rnk_cluser_city_retailer'] = apply_rank('retailer_city', rnk_vc)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['last_user_retailer_id'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    user_retailer_city_cluster = (table['last_user_city_id'].astype(np.int64) * 10000000 + \\\n",
    "        table['user_retailer_most_common'].astype(np.int64) * 10000 + \\\n",
    "        table['cluster_id'])\n",
    "    table['rnk_cluster_retailer_city2'] = user_retailer_city_cluster.map(\n",
    "        rnk_vc.set_index('cluster_retailer_city')['rnk_cluser_city_retailer']\n",
    "    ).fillna(10000).astype(np.int16)\n",
    "    del cluster_retailer_city_vc\n",
    "    del rnk_vc\n",
    "    del user_retailer_city_cluster\n",
    "    \n",
    "    short_train = train[['cluster_id', 'user_id']][\n",
    "        train.user_id.isin(users) & (~train[['ui', 'order_id']].duplicated())\n",
    "    ]\n",
    "    \n",
    "    vc = short_train['user_id'].value_counts()\n",
    "    for cluster_id in TOP_K_CLUSTERS[:40]:\n",
    "        table[f'f102_{cluster_id}'] = table.user_id.map(\n",
    "            (short_train.cluster_id == cluster_id).groupby(short_train.user_id).sum() / vc\n",
    "        ).astype(np.float16)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55808a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "\n",
    "    train_test = pd.read_parquet(TRAIN_TEST_PATH)\n",
    "    train_test['product_price'] = train_test['product_price'].astype(np.float32)\n",
    "    train_test['product_discount'] = train_test['product_discount'].astype(np.float32)\n",
    "    clusters = pd.read_parquet(CLUSTERS_PATH)\n",
    "    table = pd.read_parquet(TOPK_TEST_PATH)\n",
    "    user_decoder = pickle.load(open(USER_DECODER_PATH, 'rb'))\n",
    "    \n",
    "    for df in [train_test, table]:\n",
    "        df['ui'] = df['user_id'].astype(np.int64) * 10000 + df['cluster_id']\n",
    "        \n",
    "    print(1)\n",
    "    table = create_fit_table(train_test, table, clusters, \n",
    "                             RECS_NN_TEST_PATH, RECS_TEST_PATH)\n",
    "    print(2)\n",
    "    del clusters\n",
    "    \n",
    "    already_bought = table['ui'].isin(train_test['ui'])\n",
    "    \n",
    "    cluster_weights = get_cluster_weights(train_test)\n",
    "    del train_test\n",
    " \n",
    "    weights = table.cluster_id.map(cluster_weights.set_index('cluster_id')['w']).fillna(\n",
    "        cluster_weights['w'].max()\n",
    "    )\n",
    "    del cluster_weights\n",
    "    print(3)\n",
    "    X = table.drop(['user_id', 'ui'], axis=1).to_numpy(dtype=np.float32)\n",
    "    print(type(X[0][0]))\n",
    "    print(4)\n",
    "    users = np.array(table['user_id'])\n",
    "    items = np.array(table['cluster_id'])\n",
    "    del table\n",
    "    print(5)\n",
    "    \n",
    "    ranker_model = pickle.load(open(RANKER_MODEL_PATH, 'rb'))\n",
    "    pred = ranker_model.predict(X)\n",
    "    \n",
    "    recs = get_recs(pred, users, items, already_bought, weights)\n",
    "\n",
    "    submit = pd.DataFrame()\n",
    "    submit['user_id'] = pd.Series(recs.keys())\n",
    "    submit['id'] = user_decoder[submit['user_id']]\n",
    "\n",
    "    submit['target'] = [';'.join([str(i) for i in values]) for values in recs.values()]\n",
    "    submit[['id', 'target']].to_csv(SUBMIT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%memit predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915666d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ca1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
